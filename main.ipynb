{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import os\n",
        "import statistics\n",
        "# Define paths\n",
        "SOURCE_DIR = r\"/count/drive/MyDrive/real data/\"\n",
        "DATA_ENTRY_PATH = os.path.join(SOURCE_DIR, \"HACKATHON_CORRUPTED_Data_Entry.csv\")\n",
        "BBOX_LIST_PATH = os.path.join(SOURCE_DIR, \"HACKATHON_CORRUPTED_BBox_List.csv\")\n",
        "# Output paths\n",
        "DATA_ENTRY_OUT = \"Data_Entry_Cleaned.csv\"\n",
        "DATA_ENTRY_BAD_OUT = \"Data_Entry_Bad.csv\"\n",
        "BBox_List_OUT = \"BBox_List_Cleaned.csv\"\n",
        "BBox_List_BAD_OUT = \"BBox_List_Bad.csv\"\n",
        "REPORT_OUT = \"cleaning_report.md\"\n",
        "def clean_data():\n",
        "    report_lines = []\n",
        "    report_lines.append(\"# Data Cleaning Report\\n\")\n",
        "    # --- Data_Entry.csv Cleaning ---\n",
        "    print(f\"Cleaning Data_Entry.csv from {DATA_ENTRY_PATH}...\")\n",
        "\n",
        "    cleaned_rows = []\n",
        "    bad_rows = []\n",
        "    initial_rows = 0\n",
        "    headers = []\n",
        "    ages = []\n",
        "\n",
        "    try:\n",
        "        with open(DATA_ENTRY_PATH, 'r', newline='', encoding='utf-8') as f:\n",
        "            reader = csv.DictReader(f)\n",
        "            # Clean headers: strip spaces\n",
        "            headers = [h.strip() for h in reader.fieldnames]\n",
        "\n",
        "            # We need to read rows, but the DictReader keys are the original headers\n",
        "            # We will map them to clean headers\n",
        "            original_headers = reader.fieldnames\n",
        "            header_map = {orig: orig.strip() for orig in original_headers}\n",
        "\n",
        "            for original_row in reader:\n",
        "                # Create a row with clean keys\n",
        "                row = {header_map[k]: v for k, v in original_row.items() if k in header_map}\n",
        "\n",
        "                initial_rows += 1\n",
        "\n",
        "                # 1. Image Index Checking\n",
        "                img_idx = row.get('Image Index', '')\n",
        "                if 'invalid_image' in img_idx or 'missing_image' in img_idx:\n",
        "                    row['Rejection_Reason'] = 'Invalid Image Index'\n",
        "                    bad_rows.append(row)\n",
        "                    continue\n",
        "\n",
        "                # Standardize filename (keep original in bad row if we rejected it above, but here we process valid ones)\n",
        "                if img_idx.startswith('IMG_'):\n",
        "                    row['Image Index'] = img_idx.replace('IMG_', '')\n",
        "\n",
        "                # 2. Gender Checking\n",
        "                gender = row.get('Gender', '').strip()\n",
        "                if gender in ['female', 'female ', 'F']:\n",
        "                    row['Gender'] = 'F'\n",
        "                elif gender in ['Male', 'M']:\n",
        "                    row['Gender'] = 'M'\n",
        "                else:\n",
        "                    # Drop unknown/missing\n",
        "                    row['Rejection_Reason'] = f'Invalid/Missing Gender: {gender}'\n",
        "                    bad_rows.append(row)\n",
        "                    continue\n",
        "\n",
        "                # 3. Patient Age\n",
        "                age_str = str(row.get('patient_age', '')).upper().replace('Y', '').strip()\n",
        "                try:\n",
        "                    if age_str == 'TWENTY FIVE':\n",
        "                        age = 25.0\n",
        "                    else:\n",
        "                        age = float(age_str)\n",
        "\n",
        "                    # Store age for median calc later, but keep invalid ones for now to filter?\n",
        "                    # Actually, let's collect valid ages to calc median first?\n",
        "                    # Two pass approach or buffering?\n",
        "                    # Buffering is fine for 100k rows.\n",
        "\n",
        "                    row['patient_age_clean'] = age # Temp storage\n",
        "\n",
        "                except ValueError:\n",
        "                    row['patient_age_clean'] = None # Mark for filling\n",
        "\n",
        "                # 4. Finding Labels\n",
        "                garbage_labels = ['XYZ_Disease', '123', 'Unknown_Disorder']\n",
        "                labels = row.get('finding_labels', '').split('|')\n",
        "                clean_labels_list = [l.strip() for l in labels if l.strip() not in garbage_labels and l.strip() != 'None' and l.strip() != '']\n",
        "\n",
        "                if not clean_labels_list:\n",
        "                    row['finding_labels'] = \"No Finding\"\n",
        "                else:\n",
        "                    row['finding_labels'] = \"|\".join(clean_labels_list)\n",
        "\n",
        "                cleaned_rows.append(row)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: {DATA_ENTRY_PATH} not found.\")\n",
        "        return\n",
        "    report_lines.append(f\"## Data_Entry.csv\\n\")\n",
        "    report_lines.append(f\"- **Initial Rows**: {initial_rows}\")\n",
        "\n",
        "    # Age Processing: Median Replacement\n",
        "    # Collect valid ages (0-120)\n",
        "    valid_ages = [r['patient_age_clean'] for r in cleaned_rows if r['patient_age_clean'] is not None and 0 <= r['patient_age_clean'] <= 120]\n",
        "\n",
        "    if valid_ages:\n",
        "        median_age = int(round(statistics.median(valid_ages)))\n",
        "    else:\n",
        "        median_age = 0 # Fallback\n",
        "\n",
        "    for row in cleaned_rows:\n",
        "        age_val = row['patient_age_clean']\n",
        "        if age_val is None or not (0 <= age_val <= 120):\n",
        "            row['patient_age'] = str(median_age)\n",
        "        else:\n",
        "            row['patient_age'] = str(int(round(age_val)))\n",
        "\n",
        "        # Remove temp key\n",
        "        del row['patient_age_clean']\n",
        "    report_lines.append(f\"- **Rows after cleaning (Gender/Image)**: {len(cleaned_rows)}\")\n",
        "    report_lines.append(f\"- **Age Cleaning**: Replaced outliers with median age ({median_age}).\")\n",
        "    # Deduplication\n",
        "    # Convert dicts to unique set of tuples?\n",
        "    # Use a set of frozensets of items\n",
        "    unique_rows = []\n",
        "    seen = set()\n",
        "    for row in cleaned_rows:\n",
        "        # Create a tuple of items sorted by key to be hashable\n",
        "        row_tuple = tuple(sorted(row.items()))\n",
        "        if row_tuple not in seen:\n",
        "            seen.add(row_tuple)\n",
        "            unique_rows.append(row)\n",
        "\n",
        "    report_lines.append(f\"- **Duplicates Removed**: {len(cleaned_rows) - len(unique_rows)}\")\n",
        "    report_lines.append(f\"- **Final Rows**: {len(unique_rows)}\\n\")\n",
        "    # Write Data Entry\n",
        "    print(f\"Saving to {DATA_ENTRY_OUT}...\")\n",
        "    if unique_rows:\n",
        "        with open(DATA_ENTRY_OUT, 'w', newline='', encoding='utf-8') as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=headers)\n",
        "            writer.writeheader()\n",
        "            writer.writerows(unique_rows)\n",
        "    # Write Bad Data Entry\n",
        "    print(f\"Saving to {DATA_ENTRY_BAD_OUT}...\")\n",
        "    if bad_rows:\n",
        "        # Add Rejection_Reason to headers for bad file\n",
        "        bad_headers = headers + ['Rejection_Reason'] if headers else ['Rejection_Reason']\n",
        "        with open(DATA_ENTRY_BAD_OUT, 'w', newline='', encoding='utf-8') as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=bad_headers)\n",
        "            writer.writeheader()\n",
        "            writer.writerows(bad_rows)\n",
        "    # --- BBox_List.csv Cleaning ---\n",
        "    print(f\"Cleaning BBox_List.csv from {BBOX_LIST_PATH}...\")\n",
        "    cleaned_bbox_rows = []\n",
        "    bad_bbox_rows = []\n",
        "    initial_bbox_rows = 0\n",
        "    bbox_headers = []\n",
        "\n",
        "    try:\n",
        "        with open(BBOX_LIST_PATH, 'r', newline='', encoding='utf-8') as f:\n",
        "            reader = csv.DictReader(f)\n",
        "            bbox_headers = [h.strip() for h in reader.fieldnames] # Clean headers\n",
        "\n",
        "            # Map old headers to new clean ones if needed?\n",
        "            # The reader keys will still be the original ones (with spaces).\n",
        "            # We need to map them.\n",
        "\n",
        "            original_headers = reader.fieldnames\n",
        "\n",
        "            for row in reader:\n",
        "                initial_bbox_rows += 1\n",
        "                img_idx = row.get('Image Index', '')\n",
        "                if 'invalid_image' in img_idx or 'missing_image' in img_idx:\n",
        "                    row['Rejection_Reason'] = 'Invalid Image Index'\n",
        "                    bad_bbox_rows.append(row)\n",
        "                    continue\n",
        "\n",
        "                # Check BBox coords\n",
        "                # Keys: 'bbox_x ', 'bbox-y', 'width', 'height' (based on view_file output)\n",
        "                # Let's try to get values safely\n",
        "\n",
        "                try:\n",
        "                    x = float(row.get('bbox_x ', row.get('bbox_x', 0))) # Try with space\n",
        "                    y = float(row.get('bbox-y', row.get('bbox_y', 0)))\n",
        "                    w = float(row.get('width', 0))\n",
        "                    h = float(row.get('height', 0))\n",
        "\n",
        "                    if x >= 0 and y >= 0 and w > 0 and h > 0 and x < 5000 and y < 5000 and w < 5000 and h < 5000:\n",
        "                        # Clean Headers in the row dict for output?\n",
        "                        # Let's standarize output headers: Image Index, Finding Label, bbox_x, bbox_y, width, height\n",
        "                        new_row = {\n",
        "                            'Image Index': img_idx,\n",
        "                            'Finding Label': row.get('Finding Label', ''),\n",
        "                            'bbox_x': x,\n",
        "                            'bbox_y': y,\n",
        "                            'width': w,\n",
        "                            'height': h\n",
        "                        }\n",
        "                        cleaned_bbox_rows.append(new_row)\n",
        "                    else:\n",
        "                        row['Rejection_Reason'] = f'Invalid Coordinates: x={x}, y={y}, w={w}, h={h}'\n",
        "                        bad_bbox_rows.append(row)\n",
        "\n",
        "                except ValueError:\n",
        "                    row['Rejection_Reason'] = 'Malformed Coordinates'\n",
        "                    bad_bbox_rows.append(row)\n",
        "                    continue # Drop rows with bad parsing\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: {BBOX_LIST_PATH} not found.\")\n",
        "        report_lines.append(f\"\\nError: BBox_List.csv not found.\")\n",
        "\n",
        "    report_lines.append(f\"## BBox_List.csv\\n\")\n",
        "    report_lines.append(f\"- **Initial Rows**: {initial_bbox_rows}\")\n",
        "    report_lines.append(f\"- **Final Rows**: {len(cleaned_bbox_rows)}\")\n",
        "\n",
        "    # Write BBox List\n",
        "    print(f\"Saving to {BBox_List_OUT}...\")\n",
        "    output_bbox_headers = ['Image Index', 'Finding Label', 'bbox_x', 'bbox_y', 'width', 'height']\n",
        "    if cleaned_bbox_rows:\n",
        "        with open(BBox_List_OUT, 'w', newline='', encoding='utf-8') as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=output_bbox_headers)\n",
        "            writer.writeheader()\n",
        "            writer.writerows(cleaned_bbox_rows)\n",
        "    # Write Bad BBox List\n",
        "    print(f\"Saving to {BBox_List_BAD_OUT}...\")\n",
        "    if bad_bbox_rows:\n",
        "        # Use original headers + reason\n",
        "        bad_bbox_headers = bbox_headers + ['Rejection_Reason'] if bbox_headers else ['Rejection_Reason']\n",
        "        # We need to map keys back if we want to write full row?\n",
        "        # The bad_bbox_rows contain original keys because we appended 'row' from reader\n",
        "        # But 'row' from DictReader might have spaces in keys.\n",
        "        # We collected 'bbox_headers' cleaned earlier, let's use the keys present in the dict.\n",
        "        if bad_bbox_rows:\n",
        "             sample_keys = list(bad_bbox_rows[0].keys())\n",
        "             with open(BBox_List_BAD_OUT, 'w', newline='', encoding='utf-8') as f:\n",
        "                writer = csv.DictWriter(f, fieldnames=sample_keys)\n",
        "                writer.writeheader()\n",
        "                writer.writerows(bad_bbox_rows)\n",
        "    # Write Report\n",
        "    with open(REPORT_OUT, 'w') as f:\n",
        "        f.writelines(report_lines)\n",
        "\n",
        "    print(\"Cleaning complete.\")\n",
        "if __name__ == \"__main__\":\n",
        "    clean_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-baciWk2OS-G",
        "outputId": "0d8e3931-d13c-494e-8feb-89ba543b997b"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning Data_Entry.csv from /count/drive/MyDrive/real data/HACKATHON_CORRUPTED_Data_Entry.csv...\n",
            "Error: /count/drive/MyDrive/real data/HACKATHON_CORRUPTED_Data_Entry.csv not found.\n"
          ]
        }
      ]
    }
  ]
}